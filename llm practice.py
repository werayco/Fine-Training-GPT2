# -*- coding: utf-8 -*-
"""Practice_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BcrbleVI9YcEG5ihFAlJ1xP8ecL7NAtI

Installing the necessary **Dependencies**
"""

!pip install --upgrade langchain transformers tensorflow
!pip install torch
!pip install langchain_community
!pip install wikipedia
!pip install tensorflow
!pip install bitsandbytes>=0.39.0 -q
!pip install -U duckduckgo-search

"""### Loading Our Persona-Chat Dataset"""

# !pip install -U langchain-huggingface torch
!pip install datasets
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, pipeline
from datasets import load_dataset
file_path = "/content/trainer.csv"
dataset = load_dataset('csv', data_files={'train': file_path})

dataset["train"][0]["user 1 personas"]

!pip install wandb # installing weights and bias to track the metrics of our llm
import wandb
import os
import re


def cleaning_1(x):
    data = re.sub("\n","",x["user 1 personas"])
    data = re.sub("\d+","",data)
    data = re.sub("@","",data)
    return {"Input":data}

dataset_1 = dataset.map(cleaning_1)

def cleaning_2(x):
    data = re.sub("\n","",x["user 2 personas"])
    data = re.sub("\d+","",data)
    data = re.sub("@","",data)
    return {"Input":data}

dataseter = dataset_1.map(cleaning_2)

from transformers import AutoTokenizer, AutoModelForCausalLM
import os

access_token = "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"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2",)
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")

if tokenizer.pad_token_id is None:
  tokenizer.pad_token_id = tokenizer.eos_token_id

tokenizer.clean_up_tokenization_spaces = True

def token(x):
    tokenized_output = tokenizer(
        x["user 1 personas"],x["user 2 personas"],
        max_length=128,
        truncation=True,
        padding='max_length',
        return_tensors=None,
    )
    return tokenized_output
data = dataseter.map(token)

### Training Section
from transformers import (DataCollatorForLanguageModeling,TrainingArguments,Trainer)

args = TrainingArguments(output_dir="./fine_gpt2",per_device_eval_batch_size=8,
                         per_device_train_batch_size=8,run_name="final_tuner",num_train_epochs=10,
                         fp16=True,report_to="wandb") # this function provides the training arguments for our model

datacollator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False) # used for preping our data for batching

trainer = Trainer(model=model,args=args,data_collator=datacollator,train_dataset=data["train"])

trainer.train() # this trains your module

from huggingface_hub import notebook_login
notebook_login()

trainer.push_to_hub("heisrayco-finetuned_gpt2") # commiting our model to huggingfacehub

tokenizer.push_to_hub("tokenizer_gpt2",)

"""### **RAG Section: Using a Custom document created by ME**"""

# 4003ff4a11bfc183797d332f5cb9dc5be7743a40
!pip install pyPDF
!pip install langchain
!pip install -U langchain-community langchain_huggingface
!pip install faiss-gpu
!pip install sentence-transformers

from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import create_retrieval_chain

#RAG Section: Using the John bird pdf as an example [ I'm an engineer, don't blame me :)]
doc = PyPDFLoader(r"/content/pdf_for_rag.pdf")
documents = doc.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)
ind_texts = text_splitter.split_documents(documents)

# creating vector database using FaceBook AI Similarity Search(FAISS), you can use picone or chroma if you choose
emd = HuggingFaceEmbeddings(model_name = "sentence-transformers/all-mpnet-base-v2",model_kwargs = {'device': 'cuda'},encode_kwargs = {'normalize_embeddings': False})
db = FAISS.from_documents(documents=ind_texts,embedding=emd)

retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 6}) # for the RAG system

"""### Lets Tests the Vector Database"""

your_query = "Tensors are multidimensional arrays used to represent data in various dimensions. They are fundamental in "

needed = db.similarity_search(your_query,k=1)
answer = ",".join([d.page_content for d in needed])
print(answer)

"""### Saving my fine_tuned gpt2 model to my local directory"""

model.save_pretrained("./mymodel")

# now loading the fine tuned model from huggingface hub

pipe = pipeline("text-generation", model="heisrayco/fine_gpt2",tokenizer=tokenizer,device="cuda",max_new_tokens=100,temperature=0.7,top_k=4,no_repeat_ngram_size=3,num_return_sequences=1)
llm = HuggingFacePipeline(pipeline=pipe)

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:{context}
User's Input: {input}
""")

doc_chain = create_stuff_documents_chain(llm,prompt)
qa = create_retrieval_chain(retriever,doc_chain)

query_rag = qa.invoke({"input": "What is an agent?","context": retriever})
ans = query_rag["answer"].replace("\n","").replace("\n","") # this returns the respective answer for your input
print(ans)

# Use a pipeline as a high-level helper
from transformers import pipeline
from langchain_huggingface import HuggingFacePipeline

if tokenizer.pad_token_id is None:
  tokenizer.pad_token_id = tokenizer.eos_token_id



import re
result = llm("what is ")[0]["generated_text"].replace("\n","")
result

# prompt templating
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory()

name = input("what name: ")
country = input("what country: ")
template ="Tell a Story about {name} and who happens to live in {country}"
ppt  = PromptTemplate(template=template,input_variables=["name","country"])
query = ppt.format(name=name,country=country)
queryResult = llm(query)
queryResult